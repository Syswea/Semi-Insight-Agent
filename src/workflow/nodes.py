"""
src/workflow/nodes.py

å®šä¹‰ LangGraph çš„æ ¸å¿ƒèŠ‚ç‚¹å‡½æ•°ã€‚
å®ç°ç®€å•çš„ ReAct (Reasoning + Acting) å¾ªç¯ã€‚
"""

import json
import logging
import os
import re
from typing import Dict, Any, cast

from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

from src.state import AgentState
from src.tools.cypher_query import CypherQueryEngine
from llama_index.llms.openai_like import OpenAILike

logger = logging.getLogger(__name__)

# åˆå§‹åŒ–å·¥å…·
cypher_engine = CypherQueryEngine()

# åˆå§‹åŒ– LLM (ç”¨äºæ¨ç†è§„åˆ’)
llm = OpenAILike(
    model=os.getenv("LLM_MODEL", "qwen/qwen3-14b"),
    api_base=os.getenv("OPENAI_API_BASE", "http://127.0.0.1:1234/v1"),
    api_key=os.getenv("OPENAI_API_KEY", "lm-studio"),
    is_chat_model=True,
    timeout=300.0,
    temperature=0.0,
)


def reasoning_node(state: AgentState) -> Dict[str, Any]:
    """
    æ¨ç†èŠ‚ç‚¹ï¼šåˆ†æå½“å‰çŠ¶æ€ï¼Œå†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨ (æŸ¥å›¾è°± or æŸ¥ç½‘ç»œ or ç»“æŸ)ã€‚

    ç”±äºæœ¬åœ°æ¨¡å‹ Function Calling ä¸ç¨³å®šï¼Œè¿™é‡Œä½¿ç”¨ä¸¥æ ¼çš„ JSON æ ¼å¼æç¤ºå·¥ç¨‹ã€‚
    """
    messages = state["messages"]

    # æ„å»º Prompt
    system_prompt = (
        "You are a Semiconductor Industry Analyst Agent.\n"
        "You have access to TWO tools:\n"
        "1. `query_graph` - Query the Knowledge Graph for entities and relationships\n"
        "2. `web_search` - Search the web for real-time information\n\n"
        "--- Decision Rules ---\n"
        "- Use `query_graph` for: relationships between companies, technologies, supply chain info\n"
        "- Use `web_search` for: founding dates, HQ locations, current news, recent events\n"
        "- Use `final_answer` when you have collected sufficient information\n\n"
        "--- Format ---\n"
        "If you need more info from the knowledge graph:\n"
        '{"action": "query_graph", "query": "YOUR_QUESTION_HERE"}\n\n'
        "If you need real-time info from the web:\n"
        '{"action": "web_search", "query": "YOUR_QUESTION_HERE"}\n\n'
        "If you have enough info to answer:\n"
        '{"action": "final_answer", "content": "YOUR_FINAL_ANSWER", "requires_debate": true/false, "confidence": 0.0-1.0}\n\n'
        "--- Constraints ---\n"
        "1. Output ONLY valid JSON.\n"
        "2. Do not explain your thought process outside the JSON.\n"
        "3. Set `requires_debate` to true for investment advice, competitive analysis, or complex industry trends. Set to false for simple facts (founding dates, HQs, single metrics).\n"
    )

    # ç®€å•çš„å°†æ¶ˆæ¯è½¬æ¢ä¸ºæ–‡æœ¬ä¸Šä¸‹æ–‡
    history_str = ""
    for m in messages:
        if isinstance(m, HumanMessage):
            history_str += f"User: {m.content}\n"
        elif isinstance(m, AIMessage):
            history_str += f"Assistant: {m.content}\n"
        elif isinstance(m, SystemMessage):
            # è¿™é‡Œçš„ SystemMessage å¯èƒ½åŒ…å«å·¥å…·çš„è¿”å›ç»“æœ
            history_str += f"Tool Output: {m.content}\n"

    prompt = f"{system_prompt}\n--- History ---\n{history_str}\nNext Step (JSON):"

    try:
        raw_response = llm.complete(prompt).text.strip()

        # 1. ç§»é™¤ <think> æ ‡ç­¾
        clean_response = re.sub(
            r"<think>.*?</think>", "", raw_response, flags=re.DOTALL
        )

        # 2. æå– JSON ä»£ç å—
        if "```json" in clean_response:
            clean_response = clean_response.split("```json")[1].split("```")[0].strip()
        elif "```" in clean_response:
            clean_response = clean_response.split("```")[1].split("```")[0].strip()

        clean_response = clean_response.strip()

        decision = json.loads(cast(str, clean_response))
        return {"messages": [AIMessage(content=json.dumps(decision))]}
    except Exception as e:
        # Capture raw_response if available, otherwise use "Unknown"
        err_raw = locals().get("raw_response", "Unknown")
        logger.error(f"Reasoning failed: {e}. Raw: {err_raw}")
        # å›é€€ç­–ç•¥
        return {
            "messages": [
                AIMessage(
                    content=json.dumps(
                        {
                            "action": "final_answer",
                            "content": f"Error parsing intent: {err_raw}",
                        }
                    )
                )
            ]
        }


def tool_execution_node(state: AgentState) -> Dict[str, Any]:
    """
    æ‰§è¡ŒèŠ‚ç‚¹ï¼šè§£æä¸Šä¸€æ­¥çš„ JSON å¹¶æ‰§è¡Œå·¥å…·ã€‚

    æ”¯æŒçš„å·¥å…·ï¼š
    - query_graph: æŸ¥è¯¢çŸ¥è¯†å›¾è°± (Neo4j)
    - web_search: ç½‘ç»œæœç´¢ (DuckDuckGo via MCP)
    """
    from src.tools.web_search import web_search_tool

    last_message = state["messages"][-1]

    # Fallback to JSON parsing for our current implementation
    try:
        content = last_message.content
        if not isinstance(content, str):
            content = str(content)

        decision = json.loads(cast(str, content))
        action = decision.get("action")

        if action == "query_graph":
            query = decision.get("query")
            logger.info(f"Executing Tool: query_graph('{query}')")
            result = cypher_engine.run(query)
            return {
                "messages": [SystemMessage(content=f"Graph Search Result: {result}")]
            }

        elif action == "web_search":
            query = decision.get("query")
            logger.info(f"Executing Tool: web_search('{query}')")
            result = web_search_tool.search_web(query)
            return {"messages": [SystemMessage(content=f"Web Search Result: {result}")]}

        elif action == "final_answer":
            return {}

        return {"messages": [SystemMessage(content=f"Unknown action: {action}")]}

    except Exception as e:
        return {"messages": [SystemMessage(content=f"Tool execution failed: {e}")]}


def reflection_node(state: AgentState) -> Dict[str, Any]:
    """
    è‡ªæ£€èŠ‚ç‚¹ï¼šè¯„ä¼°å½“å‰ç­”æ¡ˆè´¨é‡ï¼Œå†³å®šæ˜¯å¦éœ€è¦é‡æ–°æ¨ç†ã€‚

    æ£€æŸ¥é¡¹ï¼š
    1. å·¥å…·è°ƒç”¨ç»“æœæ˜¯å¦æœ‰æ•ˆï¼ˆéç©ºã€éé”™è¯¯ï¼‰
    2. ç­”æ¡ˆæ˜¯å¦å®Œæ•´ï¼ˆæ˜¯å¦å›ç­”äº†ç”¨æˆ·çš„æ ¸å¿ƒé—®é¢˜ï¼‰
    3. æ˜¯å¦è¾¾åˆ°æœ€å¤§åæ€æ¬¡æ•°
    """
    messages = state["messages"]
    reflection_count = state.get("reflection_count", 0)
    max_reflections = state.get("max_reflections", 2)

    logger.info(f"ğŸ” Reflection Node: Count={reflection_count}/{max_reflections}")

    # æå–ç”¨æˆ·é—®é¢˜å’Œæœ€ç»ˆç­”æ¡ˆ
    user_question = None
    final_answer = None

    for msg in messages:
        if isinstance(msg, HumanMessage) and not user_question:
            user_question = msg.content

    for msg in reversed(messages):
        if isinstance(msg, AIMessage):
            try:
                content = msg.content
                if not isinstance(content, str):
                    content = str(content)
                clean_content = re.sub(
                    r"<think>.*?</think>", "", content, flags=re.DOTALL
                )
                decision = json.loads(clean_content)
                if decision.get("action") == "final_answer":
                    final_answer = decision.get("content")
                    break
            except:
                pass

    if not final_answer:
        logger.warning("âš ï¸ Reflection: No final answer found, skipping reflection.")
        return {
            "messages": [
                SystemMessage(content="Reflection skipped: No answer to evaluate.")
            ]
        }

    # å¦‚æœå·²è¾¾åˆ°æœ€å¤§åæ€æ¬¡æ•°ï¼Œç›´æ¥é€šè¿‡
    if reflection_count >= max_reflections:
        logger.info(
            f"âœ… Reflection: Max reflections reached ({max_reflections}), passing."
        )
        return {
            "messages": [
                SystemMessage(
                    content=f"âœ… Reflection PASSED: Maximum reflection limit reached."
                )
            ],
            "reflection_count": reflection_count + 1,
        }

    # æ„å»ºåæ€ Prompt
    prompt = (
        "You are a Quality Assurance Agent for semiconductor industry analysis.\n"
        "Evaluate the following answer based on what's AVAILABLE IN THE KNOWLEDGE GRAPH.\n\n"
        f"User Question: {user_question}\n\n"
        f"Answer: {final_answer}\n\n"
        "Evaluation Criteria (IMPORTANT):\n"
        "1. Does the answer address the user's question using available knowledge graph data?\n"
        "2. If the knowledge graph lacks certain information (e.g., founding year, HQ location), "
        "the answer should state limitations rather than invent information.\n"
        "3. Is the answer specific given the AVAILABLE data (not generic)?\n"
        "4. Does it cite concrete entities/technologies that exist in the graph?\n\n"
        "Scoring Rules:\n"
        "- PASS if the answer uses available graph data and acknowledges limitations honestly\n"
        "- FAIL only if the answer is generic, off-topic, or makes unverifiable claims\n\n"
        "Respond with JSON ONLY:\n"
        '{"pass": true, "reason": "explanation"} if acceptable\n'
        '{"pass": false, "reason": "specific issue"} if needs improvement\n\n'
        "Output JSON:"
    )

    try:
        raw_response = llm.complete(prompt).text.strip()
        logger.info(f"Reflection LLM response: {raw_response[:200]}...")

        # æ¸…æ´—å“åº”
        clean_response = re.sub(
            r"<think>.*?</think>", "", raw_response, flags=re.DOTALL
        )

        # æå– JSON
        if "```json" in clean_response:
            clean_response = clean_response.split("```json")[1].split("```")[0].strip()
        elif "```" in clean_response:
            clean_response = clean_response.split("```")[1].split("```")[0].strip()

        clean_response = clean_response.strip()
        reflection = json.loads(cast(str, clean_response))

        passed = reflection.get("pass", False)
        reason = reflection.get("reason", "No reason provided")

        if passed:
            logger.info(f"âœ… Reflection PASSED: {reason}")
            return {
                "messages": [SystemMessage(content=f"âœ… Reflection PASSED: {reason}")],
                "reflection_count": reflection_count + 1,
            }
        else:
            logger.warning(
                f"ğŸ”„ Reflection FAILED: {reason}. Requesting re-reasoning..."
            )
            return {
                "messages": [
                    SystemMessage(
                        content=f"ğŸ”„ Reflection FAILED: {reason}. Please provide a more specific answer based on knowledge graph data."
                    )
                ],
                "reflection_count": reflection_count + 1,
                "error": reason,
            }

    except Exception as e:
        logger.error(f"âŒ Reflection check failed: {e}. Defaulting to PASS.")
        return {
            "messages": [
                SystemMessage(
                    content=f"âš ï¸ Reflection check error: {e}. Proceeding with answer."
                )
            ],
            "reflection_count": reflection_count + 1,
        }
